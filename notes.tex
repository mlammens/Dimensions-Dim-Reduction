\documentclass[12pt]{article}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}

\usepackage{bm}
\usepackage{mathrsfs}
\usepackage{verbatim}
\usepackage{floatrow}
\floatsetup[table]{font=small}

%% for inline R code: if the inline code is not correctly parsed,
%% you will see a message
\newcommand{\rinline}[1]{SOMETHING WRONG WITH knitr}



\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}

\title{Notes on the Curtis-Ghosh method}

\maketitle

\section*{Identifiability and label-switching}

The model in Curtis \& Ghosh is initially described as follows:
\begin{eqnarray*}
Y_i|\bm{\beta},\sigma^2 &\sim& \mbox{N}(\bm{x}_i^T\bm{\beta}, \sigma^2) \\
\beta_j &=& \gamma_j\theta_j \\
\gamma_j &\sim& \mbox{Bern}(\pi) \\
\theta_j|\mathscr{P} &\sim& \mathscr{P}(\cdot) \\
\mathscr{P} &\sim& \mbox{DP}\left(\alpha, \Phi_{0,\eta^2}(\cdot)\right) \\
\pi &\sim& \mbox{Unif}(0, 1) \\
\sigma^2 &\sim& \mbox{InvGam}(a_\sigma, b_\sigma) \\
\eta^2 &\sim& \mbox{InvGam}(a_\eta, b_\eta) \quad .
\end{eqnarray*}
The implementation they provide approximates the Dirichlet process
prior with a discrete Dirichlet prior as follows:
\begin{eqnarray*}
Y_i|\bm{\beta},\sigma^2 &\sim& \mbox{N}(\bm{x}_i^T\bm{\beta}, \sigma^2) \\
\beta_j &=& \gamma_j\xi_{S_j} \\
\gamma_j &\sim& \mbox{Bern}(\pi) \\
S_j &\sim& \mbox{Multinom}(p_1,\dots,p_M) \\
\xi_j|\eta^2 &\sim& \mbox{N}(0, \eta^2) \\
(p_1,\dots,p_M) &\sim& \mbox{Dirichlet}(\alpha/M,\dots,\alpha/M) \\
\pi &\sim& \mbox{Unif}(0, 1) \\
\sigma^2 &\sim& \mbox{InvGam}(a_\sigma, b_\sigma) \\
\eta^2 &\sim& \mbox{InvGam}(a_\eta, b_\eta) \quad .
\end{eqnarray*}
Here's why I think there's a label-switching problem.

Let $\xi_j = \nu_j$, $j=1,\dots,M$, where $\nu_j$ are fixed
constants. Now simultaneously permute the $\xi_j$ and the $p_j$ so
that $\xi_{j'} = \nu_j$ and $p_{j'} = p_j$, $j=1,\dots,M$, and
$j'=1,\dots,M$. The distribution of $\xi_j$ is identical under the two
permutations, meaning that permutations are non-identifiable although
$\xi_j$ and $p_j$ are still identifiable. In other words, if we switch
the labeling of the clusters, the posterior remains unchanged. That's
the label-switching problem.

Here's some evidence that it is a problem. Matt drew the bulk of this
code from the Curtis and Ghosh paper. First, the R script that sets up
the data and extracts the results after {\tt JAGS} has run.

\verbatiminput{curtis_ghosh_example.R}

\noindent Now the {\tt JAGS} script that runs the analysis.

\verbatiminput{curtis_ghosh_example.jags}

\noindent Notice that in the last lines of the R code, I'm extracting
the cluster identity for the second covariate, $Ed$, and the probability
that it is included in the model at each MCMC iteration. Let's look at
how the cluster identity changes over iterations when $Ed$ is included
in the model, i.e., when $\gamma_2 > 0$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{require}\hlstd{(reshape2)}
\hlkwd{require}\hlstd{(ggplot2)}

\hlstd{prepare} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{S}\hlstd{,} \hlkwc{gamma}\hlstd{,} \hlkwc{theta}\hlstd{) \{}
  \hlcom{## first column will contain only NA, so remove}
  \hlcom{## it}
  \hlstd{S} \hlkwb{<-} \hlkwd{t}\hlstd{(S)[,} \hlopt{-}\hlnum{1}\hlstd{]}
  \hlstd{gamma} \hlkwb{<-} \hlkwd{t}\hlstd{(gamma)[,} \hlopt{-}\hlnum{1}\hlstd{]}
  \hlstd{theta} \hlkwb{<-} \hlkwd{t}\hlstd{(theta)[,} \hlopt{-}\hlnum{1}\hlstd{]}

  \hlcom{## Bad form: hardcoded for 4 replicates}
  \hlkwd{colnames}\hlstd{(S)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"1"}\hlstd{,} \hlstr{"2"}\hlstd{,} \hlstr{"3"}\hlstd{,} \hlstr{"4"}\hlstd{)}
  \hlkwd{colnames}\hlstd{(gamma)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"1"}\hlstd{,} \hlstr{"2"}\hlstd{,} \hlstr{"3"}\hlstd{,} \hlstr{"4"}\hlstd{)}
  \hlkwd{colnames}\hlstd{(theta)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"1"}\hlstd{,} \hlstr{"2"}\hlstd{,} \hlstr{"3"}\hlstd{,} \hlstr{"4"}\hlstd{)}

  \hlcom{## convert to long form}
  \hlstd{S.long} \hlkwb{<-} \hlkwd{melt}\hlstd{(S)}
  \hlstd{gamma.long} \hlkwb{<-} \hlkwd{melt}\hlstd{(gamma)}
  \hlstd{theta.long} \hlkwb{<-} \hlkwd{melt}\hlstd{(theta)}
  \hlkwd{colnames}\hlstd{(S.long)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"Iteration"}\hlstd{,} \hlstr{"Replicate"}\hlstd{,}
    \hlstr{"Cluster"}\hlstd{)}
  \hlkwd{colnames}\hlstd{(gamma.long)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"Iteration"}\hlstd{,} \hlstr{"Replicate"}\hlstd{,}
    \hlstr{"Included"}\hlstd{)}
  \hlkwd{colnames}\hlstd{(theta.long)} \hlkwb{<-} \hlkwd{c}\hlstd{(}\hlstr{"Iteration"}\hlstd{,} \hlstr{"Replicate"}\hlstd{,}
    \hlstr{"theta"}\hlstd{)}
  \hlcom{## merge into one data frame}
  \hlstd{z} \hlkwb{<-} \hlkwd{merge}\hlstd{(S.long, gamma.long)}
  \hlstd{z} \hlkwb{<-} \hlkwd{merge}\hlstd{(z, theta.long)}
  \hlstd{z}
\hlstd{\}}

\hlstd{anchored} \hlkwb{<-} \hlnum{FALSE}

\hlkwd{source}\hlstd{(}\hlstr{"curtis_ghosh_example.R"}\hlstd{)}

\hlstd{S.plot} \hlkwb{<-} \hlkwd{prepare}\hlstd{(S.matrix, gamma.matrix, theta.matrix)}

\hlstd{for.plot} \hlkwb{<-} \hlkwd{subset}\hlstd{(S.plot[,} \hlkwd{c}\hlstd{(}\hlstr{"Cluster"}\hlstd{,} \hlstr{"Replicate"}\hlstd{)],}
  \hlstd{S.plot}\hlopt{$}\hlstd{Included} \hlopt{>} \hlnum{0}\hlstd{)}
\hlstd{p} \hlkwb{<-} \hlkwd{ggplot}\hlstd{(for.plot,} \hlkwd{aes}\hlstd{(}\hlkwc{x} \hlstd{=} \hlkwd{as.factor}\hlstd{(Cluster)))} \hlopt{+}
  \hlkwd{geom_bar}\hlstd{()} \hlopt{+} \hlkwd{xlab}\hlstd{(}\hlstr{"Cluster"}\hlstd{)} \hlopt{+} \hlkwd{theme}\hlstd{(}\hlkwc{axis.text.x} \hlstd{=} \hlkwd{element_blank}\hlstd{(),}
  \hlkwc{axis.ticks.x} \hlstd{=} \hlkwd{element_blank}\hlstd{())} \hlopt{+} \hlkwd{facet_wrap}\hlstd{(}\hlopt{~}\hlstd{Replicate)}
\hlkwd{print}\hlstd{(p)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-11}
\begin{kframe}\begin{alltt}
\hlstd{for.plot} \hlkwb{<-} \hlkwd{subset}\hlstd{(S.plot[,} \hlkwd{c}\hlstd{(}\hlstr{"Replicate"}\hlstd{,} \hlstr{"theta"}\hlstd{)],}
  \hlstd{S.plot}\hlopt{$}\hlstd{Included} \hlopt{>} \hlnum{0}\hlstd{)}
\hlstd{p} \hlkwb{<-} \hlkwd{ggplot}\hlstd{(for.plot,} \hlkwd{aes}\hlstd{(}\hlkwc{x} \hlstd{= theta))} \hlopt{+} \hlkwd{geom_density}\hlstd{()} \hlopt{+}
  \hlkwd{xlab}\hlstd{(}\hlstr{"Coefficient estimate"}\hlstd{)} \hlopt{+} \hlkwd{facet_wrap}\hlstd{(}\hlopt{~}\hlstd{Replicate)}
\hlkwd{print}\hlstd{(p)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-12}

\end{knitrout}

It's also worth comparing the posterior mean of $\beta_2$ in each of
the 4 chains. The mean reported in Table~\ref{table:theta} is
conditional on the sample having been included in the model at that
iteration, i.e., $\gamma_1=1$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{require}\hlstd{(xtable)}

\hlstd{theta.means} \hlkwb{<-} \hlkwd{ddply}\hlstd{(for.plot,} \hlkwd{c}\hlstd{(}\hlstr{"Replicate"}\hlstd{),}
  \hlstd{summarize,} \hlkwc{mean} \hlstd{=} \hlkwd{mean}\hlstd{(theta),} \hlkwc{sd} \hlstd{=} \hlkwd{sd}\hlstd{(theta))}
\hlstd{theta.table} \hlkwb{<-} \hlkwd{xtable}\hlstd{(theta.means[,} \hlopt{-}\hlnum{1}\hlstd{],} \hlkwc{digits} \hlstd{=} \hlnum{3}\hlstd{,}
  \hlkwc{caption} \hlstd{=} \hlstr{"Unanchored"}\hlstd{,} \hlkwc{label} \hlstd{=} \hlstr{"table:theta"}\hlstd{)}
\hlkwd{print}\hlstd{(theta.table,} \hlkwc{file} \hlstd{=} \hlstr{"theta-table.tex"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\input{theta-table}

$\xi_j$ is the mean of the $j$th cluster. If there is label switching
within a chain, then the means of the clusters that are being switched
between should be pretty close. Table~\ref{table:xi} shows the posterior
mean for each of the clusters in each replicate.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{xi.table} \hlkwb{<-} \hlkwd{xtable}\hlstd{(xi.means,} \hlkwc{digits} \hlstd{=} \hlnum{3}\hlstd{,} \hlkwc{caption} \hlstd{=} \hlstr{"Unanchored"}\hlstd{,}
  \hlkwc{label} \hlstd{=} \hlstr{"table:xi"}\hlstd{)}
\hlkwd{print}\hlstd{(xi.table,} \hlkwc{file} \hlstd{=} \hlstr{"xi-table.tex"}\hlstd{)}

\hlstd{fit.unanchored} \hlkwb{<-} \hlstd{fit}
\end{alltt}
\end{kframe}
\end{knitrout}

\input{xi-table}

Informal experiments, i.e., playing around, suggest that one long run
with aggressive thinning might avoid the label switching problem. It
appears that any one chain ``locks in'' to a particular identification
and doesn't move (much).

\subsection*{Avoiding label switching}

Here's an attempt to avoid (or reduce) the label switching. Rather
than leaving the cluster assignment completely to {\tt JAGS}, I
partially ``anchor'' the assignment by hard-coding the coefficient for
the first covariate as belonging to the first cluster. This is the
modified {\tt JAGS} code:

\verbatiminput{curtis_ghosh_anchored.jags}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{anchored} \hlkwb{<-} \hlnum{TRUE}

\hlkwd{source}\hlstd{(}\hlstr{"curtis_ghosh_example.R"}\hlstd{)}
\hlstd{S.plot} \hlkwb{<-} \hlkwd{prepare}\hlstd{(S.matrix, gamma.matrix, theta.matrix)}

\hlstd{for.plot} \hlkwb{<-} \hlkwd{subset}\hlstd{(S.plot[,} \hlkwd{c}\hlstd{(}\hlstr{"Cluster"}\hlstd{,} \hlstr{"Replicate"}\hlstd{)],}
  \hlstd{S.plot}\hlopt{$}\hlstd{Included} \hlopt{>} \hlnum{0}\hlstd{)}
\hlstd{p} \hlkwb{<-} \hlkwd{ggplot}\hlstd{(for.plot,} \hlkwd{aes}\hlstd{(}\hlkwc{x} \hlstd{=} \hlkwd{as.factor}\hlstd{(Cluster)))} \hlopt{+}
  \hlkwd{geom_bar}\hlstd{()} \hlopt{+} \hlkwd{xlab}\hlstd{(}\hlstr{"Cluster"}\hlstd{)} \hlopt{+} \hlkwd{theme}\hlstd{(}\hlkwc{axis.text.x} \hlstd{=} \hlkwd{element_blank}\hlstd{(),}
  \hlkwc{axis.ticks.x} \hlstd{=} \hlkwd{element_blank}\hlstd{())} \hlopt{+} \hlkwd{facet_wrap}\hlstd{(}\hlopt{~}\hlstd{Replicate)}
\hlkwd{print}\hlstd{(p)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-41}
\begin{kframe}\begin{alltt}
\hlstd{for.plot} \hlkwb{<-} \hlkwd{subset}\hlstd{(S.plot[,} \hlkwd{c}\hlstd{(}\hlstr{"Replicate"}\hlstd{,} \hlstr{"theta"}\hlstd{)],}
  \hlstd{S.plot}\hlopt{$}\hlstd{Included} \hlopt{>} \hlnum{0}\hlstd{)}
\hlstd{p} \hlkwb{<-} \hlkwd{ggplot}\hlstd{(for.plot,} \hlkwd{aes}\hlstd{(}\hlkwc{x} \hlstd{= theta))} \hlopt{+} \hlkwd{geom_density}\hlstd{()} \hlopt{+}
  \hlkwd{xlab}\hlstd{(}\hlstr{"Coefficient estimate"}\hlstd{)} \hlopt{+} \hlkwd{facet_wrap}\hlstd{(}\hlopt{~}\hlstd{Replicate)}
\hlkwd{print}\hlstd{(p)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-42}

\end{knitrout}

It's also worth comparing the posterior mean of $\beta_2$ in each of
the 4 chains. The mean reported in Table~\ref{table:anchored-theta} is
conditional on the sample having been included in the model at that
iteration, i.e., $\gamma_1=1$.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{require}\hlstd{(xtable)}

\hlstd{theta.means} \hlkwb{<-} \hlkwd{ddply}\hlstd{(for.plot,} \hlkwd{c}\hlstd{(}\hlstr{"Replicate"}\hlstd{),}
  \hlstd{summarize,} \hlkwc{mean} \hlstd{=} \hlkwd{mean}\hlstd{(theta),} \hlkwc{sd} \hlstd{=} \hlkwd{sd}\hlstd{(theta))}
\hlstd{theta.table} \hlkwb{<-} \hlkwd{xtable}\hlstd{(theta.means[,} \hlopt{-}\hlnum{1}\hlstd{],} \hlkwc{digits} \hlstd{=} \hlnum{3}\hlstd{,}
  \hlkwc{caption} \hlstd{=} \hlstr{"Anchored"}\hlstd{,} \hlkwc{label} \hlstd{=} \hlstr{"table:anchored-theta"}\hlstd{)}
\hlkwd{print}\hlstd{(theta.table,} \hlkwc{file} \hlstd{=} \hlstr{"theta-table-anchored.tex"}\hlstd{)}
\end{alltt}
\end{kframe}
\end{knitrout}

\input{theta-table-anchored}

$\xi_j$ is the mean of the $j$th cluster. If there is label switching
within a chain, then the means of the clusters that are being switched
between should be pretty close. Table~\ref{table:anchored-xi} shows
the posterior mean for each of the clusters in each replicate.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{xi.table} \hlkwb{<-} \hlkwd{xtable}\hlstd{(xi.means,} \hlkwc{digits} \hlstd{=} \hlnum{3}\hlstd{,} \hlkwc{caption} \hlstd{=} \hlstr{"Anchored"}\hlstd{,}
  \hlkwc{label} \hlstd{=} \hlstr{"table:anchored-xi"}\hlstd{)}
\hlkwd{print}\hlstd{(xi.table,} \hlkwc{file} \hlstd{=} \hlstr{"xi-table-anchored.tex"}\hlstd{)}

\hlstd{fit.anchored} \hlkwb{<-} \hlstd{fit}
\end{alltt}
\end{kframe}
\end{knitrout}

\input{xi-table-anchored}

\section*{Summarizing the posterior}

We are interested in identifying those covariates that have similar
associationss with the response variable. One way of doing so is to
calculate a dissimilarity coefficient for all pairs of covariates. It
makes sense to estimate such a coefficient as the posterior mean of a
dissimilarity coefficient calculated at each step of an MCMC
simulation. The coefficient calculated at each step should have two
properties:

\begin{enumerate}

\item It should be equal to 1 unless both coefficients are included in
the model in that step.

\item It should be equal to $(\mbox{Coeff}1 -
\mbox{Coeff}2)/(\mbox{Max(Coeff)} - \mbox{Min(Coeff)})$ when both
coefficients are included.

\end{enumerate}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{dissim.iter} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{x}\hlstd{,} \hlkwc{gamma.x}\hlstd{,} \hlkwc{y}\hlstd{,} \hlkwc{gamma.y}\hlstd{,}
  \hlkwc{max.coeff}\hlstd{,} \hlkwc{min.coeff}\hlstd{) \{}
  \hlstd{sim} \hlkwb{<-} \hlnum{0}
  \hlstd{ct} \hlkwb{<-} \hlnum{0}
  \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlkwd{length}\hlstd{(x)) \{}
    \hlkwa{if} \hlstd{((gamma.x[i]} \hlopt{>} \hlnum{0}\hlstd{)} \hlopt{&&} \hlstd{(gamma.y[i]} \hlopt{>}
      \hlnum{0}\hlstd{)) \{}
      \hlstd{sim} \hlkwb{<-} \hlstd{sim} \hlopt{+} \hlkwd{abs}\hlstd{(x[i]} \hlopt{-} \hlstd{y[i])}\hlopt{/}\hlstd{(max.coeff} \hlopt{-}
        \hlstd{min.coeff)}
    \hlstd{\}} \hlkwa{else} \hlstd{\{}
      \hlstd{sim} \hlkwb{<-} \hlstd{sim} \hlopt{+} \hlnum{1}
    \hlstd{\}}
    \hlstd{ct} \hlkwb{<-} \hlstd{ct} \hlopt{+} \hlnum{1}
  \hlstd{\}}
  \hlstd{sim} \hlkwb{<-} \hlstd{sim}\hlopt{/}\hlstd{ct}
  \hlstd{sim}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

With that code in place, it's just a matter of making all of the
possible pairwise comparison after setting up the maximum difference
between coefficient estimates across the entire sample. For now, I'm
summarizing the result using UPGMA clustering, but we could also use
the UPGMA clustering to order the variables and use {\tt corplot()} to
provide a colorful visualization of the whole distance matrix.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{dissim.cluster} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{fit}\hlstd{,} \hlkwc{label}\hlstd{) \{}
  \hlstd{max.coeff} \hlkwb{<-} \hlkwd{max}\hlstd{(fit}\hlopt{$}\hlstd{BUGSoutput}\hlopt{$}\hlstd{sims.list}\hlopt{$}\hlstd{theta)}
  \hlstd{min.coeff} \hlkwb{<-} \hlkwd{min}\hlstd{(fit}\hlopt{$}\hlstd{BUGSoutput}\hlopt{$}\hlstd{sims.list}\hlopt{$}\hlstd{theta)}
  \hlstd{dissim} \hlkwb{<-} \hlkwd{matrix}\hlstd{(}\hlnum{0}\hlstd{,} \hlkwc{ncol} \hlstd{= K,} \hlkwc{nrow} \hlstd{= K)}
  \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{2}\hlopt{:}\hlstd{K) \{}
    \hlstd{dissim[i, i]} \hlkwb{<-} \hlnum{0}
    \hlkwa{for} \hlstd{(j} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{(i} \hlopt{-} \hlnum{1}\hlstd{)) \{}
      \hlstd{dissim[i, j]} \hlkwb{<-} \hlkwd{dissim.iter}\hlstd{(fit}\hlopt{$}\hlstd{BUGSoutput}\hlopt{$}\hlstd{sims.list}\hlopt{$}\hlstd{theta[,}
        \hlstd{i], fit}\hlopt{$}\hlstd{BUGSoutput}\hlopt{$}\hlstd{sims.list}\hlopt{$}\hlstd{gamma[,}
        \hlstd{i], fit}\hlopt{$}\hlstd{BUGSoutput}\hlopt{$}\hlstd{sims.list}\hlopt{$}\hlstd{theta[,}
        \hlstd{j], fit}\hlopt{$}\hlstd{BUGSoutput}\hlopt{$}\hlstd{sims.list}\hlopt{$}\hlstd{gamma[,}
        \hlstd{j], max.coeff, min.coeff)}
      \hlstd{dissim[j, i]} \hlkwb{<-} \hlstd{dissim[i, j]}
    \hlstd{\}}
  \hlstd{\}}
  \hlkwd{colnames}\hlstd{(dissim)} \hlkwb{<-} \hlstd{crime.data}
  \hlkwd{rownames}\hlstd{(dissim)} \hlkwb{<-} \hlstd{crime.data}
  \hlstd{dissim.hclust} \hlkwb{<-} \hlkwd{hclust}\hlstd{(}\hlkwd{as.dist}\hlstd{(dissim),} \hlkwc{method} \hlstd{=} \hlstr{"average"}\hlstd{)}
  \hlkwd{plot}\hlstd{(dissim.hclust,} \hlkwc{main} \hlstd{=} \hlkwd{paste}\hlstd{(}\hlstr{"Covariate clustering for"}\hlstd{,}
    \hlstd{label),} \hlkwc{sub} \hlstd{=} \hlstr{""}\hlstd{,} \hlkwc{xlab} \hlstd{=} \hlstr{""}\hlstd{,} \hlkwc{ylab} \hlstd{=} \hlstr{"Dissimilarity"}\hlstd{)}
  \hlcom{## return the dissimilarity matrix for later}
  \hlcom{## use}
  \hlstd{dissim}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

Alternatively, we could display the results in two dimensions usins
multidimensional scaling.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{dissim.MDS} \hlkwb{<-} \hlkwa{function}\hlstd{(}\hlkwc{dissim}\hlstd{,} \hlkwc{label}\hlstd{) \{}
  \hlstd{dissim.mds} \hlkwb{<-} \hlkwd{cmdscale}\hlstd{(dissim)}
  \hlkwd{plot}\hlstd{(dissim.mds,} \hlkwc{typ} \hlstd{=} \hlstr{"n"}\hlstd{,} \hlkwc{xlab} \hlstd{=} \hlstr{"Axis 1"}\hlstd{,}
    \hlkwc{ylab} \hlstd{=} \hlstr{"Axis 2"}\hlstd{,} \hlkwc{main} \hlstd{=} \hlkwd{paste}\hlstd{(}\hlstr{"MDS for "}\hlstd{,}
      \hlstd{label))}
  \hlkwd{text}\hlstd{(dissim.mds,} \hlkwd{rownames}\hlstd{(dissim))}
\hlstd{\}}
\end{alltt}
\end{kframe}
\end{knitrout}

With the functions set up, we can now compare the clustering revealed
by the two different methods to see if it makes any difference on what
clusters of covariates we might recognize.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{result} \hlkwb{<-} \hlkwd{dissim.cluster}\hlstd{(fit.unanchored,} \hlstr{"unanchored"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-101}
\begin{kframe}\begin{alltt}
\hlkwd{dissim.MDS}\hlstd{(result,} \hlstr{"unanchored"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-102}
\begin{kframe}\begin{alltt}
\hlstd{result} \hlkwb{<-} \hlkwd{dissim.cluster}\hlstd{(fit.anchored,} \hlstr{"anchored"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-103}
\begin{kframe}\begin{alltt}
\hlkwd{dissim.MDS}\hlstd{(result,} \hlstr{"anchored"}\hlstd{)}
\end{alltt}
\end{kframe}
\includegraphics[width=\maxwidth]{figure/unnamed-chunk-104}

\end{knitrout}

\end{document}
